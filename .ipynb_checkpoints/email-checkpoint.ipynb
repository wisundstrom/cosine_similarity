{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import glob\n",
    "from itertools import chain\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import string as st\n",
    "import random as ra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_files = glob.glob(\"data/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_emails(file_name):\n",
    "    \n",
    "    with open(file_name, 'rt') as message:\n",
    "        file = message.read()\n",
    "        \n",
    "    file = file.replace(\"\\n\", \" \")\n",
    "    exp = \"[\\w\\.\\+\\-\\!\\?\\*#$%&'=]+@[\\w]+\\.+[\\w-]+\"\n",
    "    emails = re.findall(exp, file)\n",
    "\n",
    "    return emails\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails = map(find_emails, txt_files)\n",
    "flattened = set(chain.from_iterable(emails))\n",
    "email_list = list(set(map(lambda x: x.strip(\"'\"), flattened)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple character vectorization of emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='char', lowercase=False)\n",
    "vectorizer = vectorizer.fit(email_list)\n",
    "email_vect = vectorizer.transform(email_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5358x74 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 78701 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = cosine_similarity(email_vect)\n",
    "n_dim, m_dim = sims.shape\n",
    "sims= np.triu(sims, k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finding most similar pairs to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5 = np.argpartition(sims, -5, axis=None)[-5:]\n",
    "n_dim, m_dim = sims.shape\n",
    "top5_indicesnp_1 = [int(x/m_dim) for x in top5]\n",
    "top5_indicesnp_2 = [x%m_dim for x in top5]\n",
    "top5_indicesnp = (tuple(top5_indicesnp_1),tuple(top5_indicesnp_2))\n",
    "top5_indices = list(map((lambda x : (int(x/m_dim), x%m_dim) ), (top5)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(330, 3093), (257, 2657), (2093, 4918), (40, 3013), (761, 2111)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top5_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims[top5_indicesnp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('robert.scheuer@enron.com', 'editor@petroleumworld.com')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_list[688],email_list[4344]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# other ideas\n",
    "\n",
    "cosine similarity is good, the trick is encoding the emails as vectors in a way that makes sense.\n",
    "there are two different things to look at:\n",
    "\n",
    "- changing tokens to something like uppercase, lowercase, symbol, number or even using dictionaries of names or words to do word, name, symbol, number\n",
    "\n",
    "- adding in some dependence on order; right now anagrams will have similarity of 1. could change tokens to ngrams or directional transitions, ie a->b, a->c, etc. would be the vector space. \n",
    "\n",
    "the next goal is to do something combining these- where we look at uppercase->lowercase->symbol etc. transitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='char', lowercase=False)\n",
    "vectorizer = vectorizer.fit(email_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab2 = {}\n",
    "for char in symbols:\n",
    "    if char in st.ascii_lowercase:\n",
    "        vocab2[char]=0\n",
    "    elif char in st.ascii_uppercase:\n",
    "        vocab2[char]=1\n",
    "    elif char in st.digits:\n",
    "        vocab2[char]=2\n",
    "    else:\n",
    "        vocab2[char]=3\n",
    "vocab2['\"'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_mapper(list_of_string, dictionary):\n",
    "    str1 = [''.join(str(e) for e in list(map(dictionary.get, string))) for string in list_of_string] \n",
    "    return str1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped = token_mapper(email_list, vocab2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer2=CountVectorizer(analyzer='char')\n",
    "# vectors2 = vectorizer2.fit_transform(mapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try with ngrams\n",
    "vectorizer2=CountVectorizer(analyzer='char', ngram_range=(2,3))\n",
    "vectors2 = vectorizer2.fit_transform(mapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5358, 78)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = cosine_similarity(vectors2)\n",
    "n_dim, m_dim = sims.shape\n",
    "sims= np.triu(sims, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.99976319, 0.99721515, ..., 0.98826355, 0.99553586,\n",
       "        0.99892149],\n",
       "       [0.        , 0.        , 0.99860194, ..., 0.98470955, 0.99325843,\n",
       "        0.99969535],\n",
       "       [0.        , 0.        , 0.        , ..., 0.97413359, 0.98576644,\n",
       "        0.99960241],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.99824508,\n",
       "        0.98011407],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.99010594],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5 = np.argpartition(sims, -5, axis=None)[-5:]\n",
    "n_dim, m_dim = sims.shape\n",
    "top5_indicesnp_1 = [int(x/m_dim) for x in top5]\n",
    "top5_indicesnp_2 = [x%m_dim for x in top5]\n",
    "top5_indicesnp = (tuple(top5_indicesnp_1),tuple(top5_indicesnp_2))\n",
    "top5_indices = list(map((lambda x : (int(x/m_dim), x%m_dim) ), (top5)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(285, 5158), (285, 4947), (3421, 5158), (4947, 5158), (285, 3421)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top5_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims[top5_indicesnp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('IMCEANOTES-Lou+20Moore+20+3Clmoore+40houstontech+2Eorg+3E+40ENRON@ENRON.com',\n",
       " 'IMCEANOTES-Carol+20Wallen+20+3Ccwallen+40hilcorp+2Ecom+3E+40ENRON@ENRON.com')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_list[285],email_list[4947]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# moving on to words and names\n",
    " this method seems to be showing promise on this testing data, we will need to do more in-depth checking and testing to gauge performance\n",
    " the main problem now is that the measurements seem to be pretty affected by the length of the email address. using dictionaries and/or lists of names will help with this.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"NamesAll.dic\", 'rt', encoding='cp1252') as message:\n",
    "        file = message.read()\n",
    "file = file.replace(\"\\n\", \",\").replace(\" \",\"\")\n",
    "names = file.split(sep=',')\n",
    "names = list(filter(lambda x: len(x)>3, names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_add = \"abashir22@gmail.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_replace(string, dict):\n",
    "    for word in dict:\n",
    "        string2 = string.replace(word, '\"')\n",
    "        if string2 != string:\n",
    "            return string2\n",
    "        \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ra.shuffle(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#name_replace(test_add, names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped1 = [name_replace(email, names) for email in email_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped2 = token_mapper(mapped1, vocab2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try with ngrams\n",
    "vectorizer2=CountVectorizer(analyzer='char', ngram_range=(5,5))\n",
    "vectorizer2 = vectorizer2.fit(mapped2)\n",
    "vectors2 = vectorizer2.transform(mapped2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5358, 698)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(zip(vectorizer2.get_feature_names(), list(range(309))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearby_names = list(filter(lambda x : \"4\" in x[0], cols))\n",
    "nn_indices = list(map(lambda x : x[1], nearby_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5358, 125)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors3 = vectors2[:,nn_indices]\n",
    "vectors3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = cosine_similarity(vectors3)\n",
    "n_dim, m_dim = sims.shape\n",
    "sims= np.triu(sims, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5 = np.argpartition(sims, -8, axis=None)[-8:]\n",
    "n_dim, m_dim = sims.shape\n",
    "top5_indicesnp_1 = [int(x/m_dim) for x in top5]\n",
    "top5_indicesnp_2 = [x%m_dim for x in top5]\n",
    "top5_indicesnp = (tuple(top5_indicesnp_1),tuple(top5_indicesnp_2))\n",
    "top5_indices = list(map((lambda x : (int(x/m_dim), x%m_dim) ), (top5)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2958, 4526),\n",
       " (2958, 4531),\n",
       " (2958, 4522),\n",
       " (2958, 4535),\n",
       " (2958, 4568),\n",
       " (2958, 4561),\n",
       " (2958, 4567),\n",
       " (2300, 2785)]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top5_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims[top5_indicesnp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('wes.dempsey@enron.com', 'ken.rice@enron.com')"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_list[2958],email_list[4526]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is quite crude at this point, but the dictionary approach seems to be doing what we wanted it to do. \n",
    "the dictionary search through the email addressess is very inefficient, but on the irs data we could just use any names on the tax return as a dictionary, which would probably be easy. we would run into some rich, richard, dick matching problems, but for a crude first pass it would be interesting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
